from Bio import Entrez
from Bio import Medline
import pandas as pd
import numpy as np
import csv
import os
import re

def scraping_pubmed(term, email):
    search_term = term
    Entrez.email = email
    
    handle = Entrez.egquery(term=search_term)
    record = Entrez.read(handle)
    for row in record["eGQueryResult"]:
        if row['DbName']=="pubmed":
            paper_numbers=row["Count"]
            #print("{} paper total counts: ".format(search_term), row["Count"])
            
    handle = Entrez.esearch(db="pubmed",
                            term=search_term,
                            retmax=paper_numbers,
                            #mindate = '2000/01/01',
                            #maxdate = '2015/12/31'
                            )
    record = Entrez.read(handle)
    handle.close()
    idlist=record["IdList"]
    
    
    records = []
    for i in range(0, len(idlist), 10000):# A maximum of 10,000 results are available from PubMed. 
        j = i + 10000
        if j >= len(idlist):
            j = len(idlist)
            
        handle=Entrez.efetch(db="pubmed", id=idlist[i:j], rettype='medline', retmode='text')
        record=Medline.parse(handle)
        #record=list(record)
        #print(len(record))  
        
        for r in record:
            records.append(r)
            
    header = ['PMID', 'Title', 'Abstract', 'Key_words', 'Authors', 'Journal', 'Year', 'Month', 'Source','Country']
    
    with open('Processing.csv', 'w', newline='') as f:
        writer = csv.writer(f)
        
        # write the header
        writer.writerow(header)
        
        # write the data
        for paper in records:
            try:
                PMID = paper['PMID']
            except:
                PMID = None
                
            try:
                Title = paper['TI']
            except:
                try:
                    Title = paper['TT'] 
                except:
                    try:
                        Title = paper['BTI']
                    except:
                        Title = None
            
            try:
                Abstract = paper['AB']
            except:
                Abstract = None
            
            try:
                Key_words = paper['MH']
            except:
                try:
                    Key_words = paper['OT']
                except:
                    Key_words = None
                    
            try:
                Authors = paper['FAU']
            except:
                try:
                    Authors = paper['FED']
                except:
                    Authors = None
                
            try:
                Journal = paper['TA']
            except:
                Journal = None
        
            try:
                Year = paper['EDAT'].split('/')[0]
            except:
                Year = None
            
            try:
                Month = paper['EDAT'].split('/')[1]
            except:
                Month = None
            
            try:
                Source = paper['SO']
            except:
                Source = None
            
            try:
                Country = paper['AD'][0].split(',')[-1][:-1].lstrip()
                regex = re.compile('[\w\.-]+@[\w\.-]+(\.[\w]+)+')
                if re.search(regex, Country):
                    Country = Country.split(".")[0]
                else:
                    Country = Country
                
                #Country = paper['AD'][0].split(',')[-1][:-1].split('.')[0].lstrip()
            except:
                Country = None
            
            data = [PMID, Title, Abstract, Key_words, Authors, Journal, Year, Month, Source, Country]
            writer.writerow(data)
            
    
    df = pd.read_csv('Processing.csv')
    print('A total of {} articles were retrieved from PubMed by searching the term "{}".'.format(df.shape[0], term))
     
    filename2=format('_'.join(search_term.split(' '))) + '.csv'
    if os.path.exists(filename2):
        os.remove(filename2)
        os.rename('Processing.csv',filename2)
    else:
        os.rename('Processing.csv',filename2)
    
import pathlib
import pandas as pd
from collections import Counter
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import csv
import os
import seaborn as sns
import re

class SearchArticle:
    
    
    def __init__(self, df, term):
        self.df = df
        self.term = term 
        
        file_name = self.df        
        current_path = pathlib.Path().resolve()
        file_path = str(current_path) + '/' + format('_'.join(file_name.split(' '))) + '.csv'
        data = pd.read_csv(file_path)
        self.data = data
    
    def get_article_by_journal(self):
        #find articles according journal name
    
        key = self.term
        data = self.data
        pattern = re.compile(r'^\b%s\b$' % key, re.IGNORECASE)
        #articles = data[data['Journal'].str.contains(key, na=False)]
        articles = data[data['Journal'].str.match(pattern, key, na=False)]
        return articles
    
        
    def get_article_by_author(self):
        #find articles according author name (Last, First)
     
        key = self.term
        data = self.data
        articles = data[data['Authors'].str.contains(key, case = False, na=False)]
        #print(articles)  
        return articles
    
    def get_article_by_country(self):
        #find articles according author name (Last, First)
     
        key = self.term
        data = self.data
        if key.lower() in [x.lower() for x in ['US', 'USA', 'United States', 'United States of America']]:
            pattern = "USA|United States|United States of America"
            articles = data[data['Country'].str.contains(pattern, case = False, na=False)]
        elif key.lower() in [x.lower() for x in ['UK', 'United Kingdom']]:
            pattern = "UK|United Kingdom|U.K"
            articles = data[data['Country'].str.contains(pattern, case = False, na=False)]
        else:
            articles = data[data['Country'].str.contains(key, case = False, na=False)]
        #print('A total of {} articles were published by authors from {}.'.format(articles.shape[0], key.upper()))  
        return articles

class StatAnalysis:
    
    def __init__(self, df):
        self.df = df
        file_name = self.df
        current_path = pathlib.Path().resolve()
        file_path = str(current_path) + '/' + format('_'.join(file_name.split(' '))) + '.csv'
        data = pd.read_csv(file_path)
        self.data = data
        
    def descriptive_per_month(self):
        df = self.data

        #generate the summary statistics for the publication numbers per month, including mean, SD, range, median, 1st to 3rd quartile,
        df_year_month = df.groupby(['Month', 'Year']).size().reset_index(name='paper counts')
        #ds = df_year_month.groupby('Month')['paper counts'].agg('describe')
        ds = df_year_month.groupby('Month')['paper counts'].agg(['count','mean', 'std','min', 'max'])
        ds['range']= df_year_month.groupby('Month').apply(lambda x: x['paper counts'].max() - x['paper counts'].min())
        ds['median']= df_year_month.groupby('Month').apply(lambda x: x['paper counts'].median())
        ds['1st quantile']= df_year_month.groupby('Month').apply(lambda x: x['paper counts'].quantile(0.25))
        ds['3rd quantile']= df_year_month.groupby('Month').apply(lambda x: x['paper counts'].quantile(0.75))
        
        fig, ax = plt.subplots(figsize=(12,8))
        sns.set_theme(style="whitegrid")
        #sns.set(font_scale=20)
        ax = sns.boxplot(x="Month", y="paper counts", data=df_year_month)
        ax = sns.swarmplot(x="Month", y="paper counts", data=df_year_month, color=".25", size = 2)
        ax.set_xlabel("Month", fontsize = 20)
        ax.set_ylabel("Article counts", fontsize = 20)
    
        return ds

class TextAnalysis:
    
    def __init__(self, df, term):
        self.df = df
        self.term = term 
        
        file_name = self.df        
        current_path = pathlib.Path().resolve()
        file_path = str(current_path) + '/' + format('_'.join(file_name.split(' '))) + '.csv'
        data = pd.read_csv(file_path)
        self.data = data
    
    def WordCloud(self):
        #Word cloud analysis for article abstracts 
        df = self.data
        key = self.term
        
        comment_words = ''
        stopwords = set(STOPWORDS)
        #stopwords.update("abstract", "study", "demonstrated", "conclusion", "patient", "the") 
        stop_words = ["abstract", "studies", "demonstrated", "conclusion", "patient", "patients", 
                      "found", "results", "showed", "used", "using", "identified", "show"] + list(stopwords)

        
        m1 = df[key].isin(stop_words)
        m2 = df[key].str.len() < 2

        s = df.loc[~(m1 | m2), key]
        
        
        # iterate through the csv file
        for val in s:
        #for val in df[key]:
            #
            # typecaste each val to string
            val = str(val)
 
            # split the value
            tokens = val.split()
     
            # Converts each token into lowercase
            for i in range(len(tokens)):
                tokens[i] = tokens[i].lower()
                
            
     
            comment_words += " ".join(tokens)+" "
    
        wordcloud = WordCloud(width = 800, height = 800,
                              background_color ='white',
                              stopwords = stop_words,
                              min_font_size = 10).generate(comment_words)
 
        # plot the WordCloud image                      
        plt.figure(figsize = (8, 8), facecolor = None)
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.tight_layout(pad = 0)
 
        plt.show()
    
    
       

    
class Counts:
    
    def __init__(self, df, number):
        self.df = df
        #self.term = term 
        self.number = number
        
        file_name = self.df
        current_path = pathlib.Path().resolve()
        file_path = str(current_path) + '/' + format('_'.join(file_name.split(' '))) + '.csv'
        data = pd.read_csv(file_path)
        self.data = data
    
    def Author_article_counts(self):
        #list the authors with most published articles
        df = self.data
        number = self.number
    
        df.dropna(subset=['Authors'], inplace=True)
        authors_combined = df["Authors"].values.tolist()
    
        authors_list=[]

        for authors in authors_combined:
            authors=str(authors)
            authors=authors.replace("[", "")
            authors=authors.replace("]", "")
            authors=authors.split("',")
            for author in authors:
                author=author.replace("'", "")
                author=author.lstrip()
                authors_list.append(author)
        
        author_counts = Counter(authors_list)   
        return(author_counts.most_common(number)) 
    
    def journal_counts(self):
        #list the journals which published most articles
        df = self.data
        number = self.number
    
        df.dropna(subset=['Journal'], inplace=True)
        journals_combined = df["Journal"].values.tolist()
        journal_counts = Counter(journals_combined)
              
        return(journal_counts.most_common(number))

    def country_counts(self):
        #list the journals which published most articles
        df = self.data
        number = self.number
    
        df.dropna(subset=['Country'], inplace=True)
        countries_combined = df["Country"].values.tolist()
        
        USA_states =['Alabama', 'AL', 'Alaska', 'AK', 'Arizona', 'AZ', 'Arkansas', 'AR', 'California', 'CA', 'Colorado',
                     'CO', 'Connecticut', 'CT', 'Delaware', 'DE', 'Florida', 'FL', 'Georgia', 'GA', 'Hawaii', 'HI', 'Idaho',
                     'ID', 'Illinois', 'IL', 'Indiana', 'IN', 'Iowa', 'IA', 'Kansas', 'KS', 'Kentucky', 'KY', 'Louisiana', 
                     'LA', 'Maine', 'ME', 'Maryland', 'MD', 'Massachusetts', 'MA', 'Michigan', 'MI', 'Minnesota', 'MN', 
                     'Mississippi', 'MS', 'Missouri', 'MO', 'Montana', 'MT', 'Nebraska', 'NE', 'Nevada', 'NV',
                     'New Hampshire', 'NH', 'New Jersey', 'NJ', 'New Mexico', 'NM', 'New York', 'NY', 'North Carolina', 
                     'NC', 'North Dakota', 'ND', 'Ohio', 'OH', 'Oklahoma', 'OK', 'Oregon', 'OR', 'Pennsylvania', 'PA', 
                     'Rhode Island', 'RI', 'South Carolina', 'SC', 'South Dakota', 'SD', 'Tennessee', 'TN', 'Texas', 'TX',
                     'Utah', 'UT', 'Vermont', 'VT', 'Virginia', 'VA', 'Washington', 'WA', 'West Virginia', 'WV',
                     'Wisconsin', 'WI', 'Wyoming', 'WY']
        for i, country in enumerate(countries_combined):
            if country.lower() in [x.lower() for x in (df[df['Country'].str.contains('United States', na=False)]['Country'].unique().tolist()) + 
                                  (df[df['Country'].str.contains('US', na=False)]['Country'].unique().tolist()) + USA_states]:
                countries_combined[i] = 'USA'
            elif country.lower() in [x.lower() for x in ['UK', 'United Kingdom', 'U.K']]:
                countries_combined[i] = 'UK'
            elif country.lower() in [x.lower() for x in (df[df['Country'].str.contains('China', na=False)]['Country'].unique().tolist())]:
                countries_combined[i] = 'China'
            elif country.lower() in [x.lower() for x in ['Russia', 'Russian Federation', ]]:
                countries_combined[i] = 'Russia'
            else:
                countries_combined[i] = countries_combined[i]
        
        country_counts = Counter(countries_combined)
              
        return(country_counts.most_common(number))    

if __name__ == "__main__":
    term = input("Your search term: " )
    email = input("Your email: " )
    scraping_pubmed(term, email)